{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e501615",
   "metadata": {},
   "source": [
    "<h1><center>This is a notebook, which illustrates the usage of ukrainina_from_russian module</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba5bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from pymystem3 import Mystem\n",
    "import plotly.express as px\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from sys import getsizeof\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "UKR_CHANNELS = [\n",
    "    '–¢—Ä—É—Ö–∞‚ö°Ô∏è–£–∫—Ä–∞–∏–Ω–∞', '–õ–∞—á–µ–Ω –ø–∏—à–µ—Ç', '–£–∫—Ä–∞–∏–Ω—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞. –ì–ª–∞–≤–Ω–æ–µ',\n",
    "    '–í—ã —Ö–æ—Ç–∏—Ç–µ –∫–∞–∫ –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ?', '–ë–æ—Ä–∏—Å –§—ñ–ª–∞—Ç–æ–≤', 'RAGNAROCK PRIVET',\n",
    "    '–£–ù–ò–ê–ù - –Ω–æ–≤–æ—Å—Ç–∏ –£–∫—Ä–∞–∏–Ω—ã | –≤–æ–π–Ω–∞ —Å –†–æ—Å—Å–∏–µ–π | –Ω–æ–≤–∏–Ω–∏ –£–∫—Ä–∞—ó–Ω–∏ | –≤—ñ–π–Ω–∞ –∑ –†–æ—Å—ñ—î—é',\n",
    "    '–£–∫—Ä–∞–∏–Ω–∞ 24/7 –ù–æ–≤–æ—Å—Ç–∏ | –í–æ–π–Ω–∞ | –ù–æ–≤–∏–Ω–∏', '–ë—ã—Ç—å –ò–ª–∏',\n",
    "    '–£–∫—Ä–∞–∏–Ω–∞ –°–µ–π—á–∞—Å: –Ω–æ–≤–æ—Å—Ç–∏, –≤–æ–π–Ω–∞, –†–æ—Å—Å–∏—è'\n",
    "]\n",
    "\n",
    "UKR_LETTERS = ['—ó', '—î', '“ë', '—ñ']\n",
    "\n",
    "CHEAT_WORDS = [\n",
    "    '03', '04', '05', '1378', '2022', '3801', '3806', '4149', '4276',\n",
    "    '4279', '9521', '9842', 'akimapachev', 'amp', 'anna', 'com',\n",
    "    'daily', 'diza', 'donbass', 'epoddubny', 'https', 'index', 'me',\n",
    "    'news', 'opersvodki', 'pravda', 'rus', 'rvvoenkor', 'sashakots',\n",
    "    'ua', 'wargonzo', 'www', 'www pravda', '–º–∏–¥', '—Ç—Ä—É—Ö–∞', '—Ç—Ä—É—Ö–∞ —É–∫—Ä–∞–∏–Ω–∞',\n",
    "    '—É–∫—Ä–∞–∏–Ω–∞ —Å–µ–π—á–∞—Å', 'pravda com', 'daily news', 'com ua', 'https www',\n",
    "    'me rvvoenkor', 'rus news', 'ua rus', 'wargonzo –Ω–∞—à'\n",
    "]\n",
    "\n",
    "def time_decorator(function):\n",
    "    from time import time\n",
    "    def inner(*args, **kwargs):\n",
    "        start = time()\n",
    "        result = function(*args, **kwargs)\n",
    "        elapsed_time = round(time() - start, 2)\n",
    "        output = f'{function.__name__} took {elapsed_time} seconds.'\n",
    "        print(output)\n",
    "        return result\n",
    "    return inner\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, data=None):\n",
    "        \"\"\"\n",
    "        A class for the preprocessing purposes. Main methods icnludes:\n",
    "        reading, cleaning, lemmatizing and vectorizing the data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.lemmas = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.ukr_train = None\n",
    "        self.ukr_test = None\n",
    "        self.channel_train = None\n",
    "        self.channel_test = None\n",
    "        self.percent_ukr = 0\n",
    "        self.percent_rus = 1\n",
    "        self.lemmatized = False\n",
    "        self.vectorized = False\n",
    "        self.cheat_words = CHEAT_WORDS\n",
    "    \n",
    "    @time_decorator\n",
    "    def read_data(self, filename='random_msgs.csv', sep='¬∂‚àÜ',\n",
    "                  header=None):\n",
    "        \"\"\"\n",
    "        Reads the csv file into 4 columns:\n",
    "        channel\n",
    "        date of publication\n",
    "        message\n",
    "        ukrainian - 1 if ukrainian channel, 0 - otherwise.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            self.data = pd.read_csv(filename, sep=sep, header=header)\n",
    "            self.data.columns = ['channel', 'date', 'msg']\n",
    "            self.data['ukrainian'] = self.data['channel'].\\\n",
    "            apply(lambda x: 1 if x in UKR_CHANNELS else 0)\n",
    "            self.data['ukrainian'] = self.data['ukrainian'].astype('int8')\n",
    "            self.data = self.data[self.data['channel'] != '–≤–µ—á–µ—Ä—è—î–º–æ']\n",
    "            self.percent_ukr = self.data['ukrainian'].mean()\n",
    "            self.percent_rus = 1 - self.percent_ukr\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Method to get the df.\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "    \n",
    "    def get_percents_ukr_rus(self):\n",
    "        \"\"\"\n",
    "        Method to get the percentage of ukrainian and russian messages among\n",
    "        the dataset.\n",
    "        \"\"\"\n",
    "        return self.percent_ukr, self.percent_rus\n",
    "    \n",
    "    @time_decorator\n",
    "    def preprocess(self, remove_ukr_msgs=True, cut_less_than=18):\n",
    "        \"\"\"\n",
    "        This method:\n",
    "        removes short messages (with less than 18 characters);\n",
    "        removes messages with ukrainian letters.\n",
    "        \"\"\"\n",
    "        if remove_ukr_msgs:\n",
    "            for letter in UKR_LETTERS:\n",
    "                self.data = self.data[self.data['msg'].str.lower().\\\n",
    "                                        str.contains(letter) == False]\n",
    "        self.data = self.data[self.data['msg'].str.len() > cut_less_than]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.percent_ukr = self.data['ukrainian'].mean()\n",
    "        self.percent_rus = 1 - self.percent_ukr\n",
    "    \n",
    "    @time_decorator\n",
    "    def lemmatize(self, *sentences):\n",
    "        \"\"\"\n",
    "        This method has 2 usages:\n",
    "        internal; i.e. to lemmatize all messages in the dataset. Runs about 2.5\n",
    "        minutes.\n",
    "        outside; to lemmatize a given sequence of sentences.\n",
    "        \"\"\"\n",
    "        mystem = Mystem()\n",
    "        if not sentences:\n",
    "            if not self.lemmatized:\n",
    "                def preprocess_text(text):\n",
    "                    tokens = mystem.lemmatize(text.lower())\n",
    "                    text = \" \".join(tokens)\n",
    "                    return text\n",
    "\n",
    "                self.data['msg'] = self.data['msg'].apply(preprocess_text)\n",
    "                self.lemmas = self.data['msg'].copy()\n",
    "                self.lemmatized = True\n",
    "        else:\n",
    "            result = []\n",
    "            for sentence in sentences:\n",
    "                tokens = mystem.lemmatize(sentence.lower())\n",
    "                result.append(' '.join(tokens))\n",
    "            return result\n",
    "    \n",
    "    def get_lemmas(self):\n",
    "        \"\"\"\n",
    "        Method to get lemmatized messages.\n",
    "        \"\"\"\n",
    "        return self.lemmas\n",
    "    \n",
    "    def train_test_split(self, random_state=1, train_size=.8):\n",
    "        \"\"\"\n",
    "        This method clones scikit-learn train_test_split.\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.ukr_train, self.ukr_test,\\\n",
    "        self.channel_train, self.channel_test = \\\n",
    "        train_test_split(\n",
    "            self.data['msg'], self.data['ukrainian'], self.data['channel'],\n",
    "            random_state=random_state, train_size=train_size\n",
    "        )\n",
    "    \n",
    "    def get_train_test_split(self):\n",
    "        \"\"\"\n",
    "        Returns the train and test part.\n",
    "        \"\"\"\n",
    "        return self.X_train, self.X_test, self.ukr_train, self.ukr_test,\\\n",
    "        self.channel_train, self.channel_test\n",
    "    \n",
    "    @time_decorator\n",
    "    def vectorize(self, ngram_range=(1,1), sublinear_tf=True, binary=False):\n",
    "        \"\"\"\n",
    "        This method creates a pipeline of CountVectorizer() and TfidfTransformer().\n",
    "        If CountVectorizer is needed - use count_transform method.\n",
    "        If TfidfVectorizer is needed - just call a tfidf_transform method.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.vectorized:\n",
    "                self.tfidf = Pipeline([\n",
    "                    ('vect', CountVectorizer(binary=binary, ngram_range=ngram_range)),\n",
    "                    ('tfidf', TfidfTransformer(sublinear_tf=sublinear_tf))\n",
    "                ]).fit(self.X_train)\n",
    "                self.vect = self.tfidf['vect']\n",
    "                self.vectorized = True\n",
    "        except TypeError:\n",
    "            print(\"You didn't initialize data or train_test_split.\")\n",
    "        \n",
    "    \n",
    "    def get_vectorizer(self, tfidf=True):\n",
    "        \"\"\"\n",
    "        Returns the actual vectorizer.\n",
    "        \"\"\"\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @time_decorator\n",
    "    def tfidf_transform(self):\n",
    "        \"\"\"\n",
    "        Applies TfidfTransform to data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vectorizer = self.tfidf\n",
    "            X_train = self.X_train = self.vectorizer.transform(self.X_train).T\n",
    "            X_test =  self.X_test = self.vectorizer.transform(self.X_test).T\n",
    "            return X_train, X_test\n",
    "        except AttributeError:\n",
    "            print(\"You didn't initialize read_data, train_test_split or vectorize.\")\n",
    "    \n",
    "    @time_decorator\n",
    "    def count_transform(self):\n",
    "        \"\"\"\n",
    "        Applies CountTransform to data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vectorizer = self.vect\n",
    "            X_train = self.X_train = self.vectorizer.transform(self.X_train).asfptype().T\n",
    "            X_test = self.X_test = self.vectorizer.transform(self.X_test).asfptype().T\n",
    "            return X_train, X_test\n",
    "        except AttributeError:\n",
    "            print(\"You didn't initialize read_data, train_test_split or vectorize.\")\n",
    "    \n",
    "    @time_decorator\n",
    "    def remove_cheat_words(self, method='manual', freq_pivot=.5,\n",
    "                           cheat_words=CHEAT_WORDS):\n",
    "        \"\"\"\n",
    "        Removes cheat_words, like channel tags, social media links or\n",
    "        authors names.\n",
    "        \"\"\"\n",
    "        if method == 'manual':\n",
    "            delete_mask = np.zeros(self.X_train.shape[0], dtype=bool)\n",
    "            delete_mask[np.isin(np.array(\n",
    "                    self.vectorizer.get_feature_names_out()), cheat_words)\n",
    "            ] = True\n",
    "            self.X_train = self.X_train.T[:, ~delete_mask].T\n",
    "            self.X_test = self.X_test.T[:, ~delete_mask].T\n",
    "            self.delete_mask = delete_mask\n",
    "            self.cheat_words = np.array(\n",
    "                self.vectorizer.get_feature_names_out()\n",
    "            ).T[delete_mask]\n",
    "        else:\n",
    "            delete_mask = np.zeros(self.X_train.shape[0], dtype=bool)\n",
    "            for channel in self.channel_trainchannel_train.unique():\n",
    "                arr = self.X_train.T[self.channel_train == channel]\n",
    "                delete_mask |= np.array((np.sum(arr > 0, axis=0) / arr.shape[0]) > .5)[0]\n",
    "\n",
    "            self.X_train = self.X_train.T[:, ~delete_mask].T\n",
    "            self.X_test = self.X_test.T[:, ~delete_mask].T\n",
    "            self.delete_mask = delete_mask\n",
    "            self.cheat_words = np.array(\n",
    "                self.vectorizer.get_feature_names_out()\n",
    "            ).T[delete_mask]\n",
    "\n",
    "    def get_cheat_words(self):\n",
    "        \"\"\"\n",
    "        Returns the deleted cheat_words.\n",
    "        \"\"\"\n",
    "        return self.cheat_words\n",
    "    \n",
    "    def get_delete_mask(self):\n",
    "        \"\"\"\n",
    "        Returns the mask of cheat_words, which can be applied onto vectorizer matrix.\n",
    "        \"\"\"\n",
    "        return self.delete_mask\n",
    "\n",
    "class Predictor:\n",
    "    \n",
    "    def __init__(self, SVD=[None, None, None]):\n",
    "        self.Terms, self.S, self.Documents = SVD\n",
    "        if not self.S:\n",
    "            self.calculated_svd = False\n",
    "        else:\n",
    "            self.calculated_svd = True\n",
    "    \n",
    "    def get_SVD(self):\n",
    "        if self.calculated_svd:\n",
    "            return self.Terms, self.S, self.Documents\n",
    "        return 'You need to calculate SVD first'\n",
    "    \n",
    "    @time_decorator\n",
    "    def train_LSA(self, X_train, ukr_train, k=150):\n",
    "        if not self.calculated_svd:\n",
    "            self.Terms, self.S, self.Documents = svds(X_train, k=k)\n",
    "            self.ukr_centre = np.array([np.mean(self.Documents.T[ukr_train == 1], axis=0)])\n",
    "            self.rus_centre = np.array([np.mean(self.Documents.T[ukr_train == 0], axis=0)])\n",
    "            self.calculated_svd = True\n",
    "    \n",
    "    @time_decorator\n",
    "    def predict_LSA(self, X_pred):\n",
    "        Documents_pred = np.diag(1 / self.S) @ self.Terms.T @ X_pred\n",
    "        dist_to_ukr = cdist(self.ukr_centre, Documents_pred.T, metric='euclidean')[0]\n",
    "        dist_to_rus = cdist(self.rus_centre, Documents_pred.T, metric='euclidean')[0]\n",
    "        ukr_pred = self.ukr_pred = np.array([dist_to_ukr < dist_to_rus]).reshape((-1, 1))\n",
    "        return ukr_pred\n",
    "    \n",
    "    def evaluate(self, ukr_test):\n",
    "        ukr_test = np.array(ukr_test).astype(bool).reshape((-1, 1))\n",
    "        self.accuracy = round(100 * np.sum(self.ukr_pred == ukr_test) / len(ukr_test), 2)\n",
    "        return self.accuracy\n",
    "    \n",
    "    def train_NBC(self, X_train, ukr_train, percent_ukr=0):\n",
    "        '''\n",
    "        Need to use CountVectorizer(binary=True) for this one.\n",
    "        '''\n",
    "        self.terms_prob_ukr = np.mean(X_train.T[ukr_train == 1], axis=0)\n",
    "        self.terms_prob_rus = np.mean(X_train.T[ukr_train == 0], axis=0)\n",
    "        self.percent_ukr = percent_ukr\n",
    "        self.percent_rus = 1 - percent_ukr\n",
    "    \n",
    "    def predict_NBC(self, X_pred):\n",
    "        self.ukr_prob = self.percent_ukr * X_pred.T * self.terms_prob_ukr.T\n",
    "        self.rus_prob = self.percent_rus * X_pred.T * self.terms_prob_rus.T\n",
    "        ukr_pred = self.ukr_pred = self.ukr_prob > self.rus_prob\n",
    "\n",
    "    def train_LR(self, X_train, ukr_train):\n",
    "        self.logistic_regression = LogisticRegression(random_state=1).fit(X_train.T, ukr_train)\n",
    "    \n",
    "    def predict_LR(self, X_pred):\n",
    "        ukr_pred = self.ukr_pred = np.array([self.logistic_regression.predict(X_pred.T)]).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0fcbb",
   "metadata": {},
   "source": [
    "# An example of using Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fef7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_data took 0.61 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>date</th>\n",
       "      <th>msg</th>\n",
       "      <th>ukrainian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73112</th>\n",
       "      <td>–£–∫—Ä–∞–∏–Ω—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞. –ì–ª–∞–≤–Ω–æ–µ</td>\n",
       "      <td>2022-05-15 14:45:48+00:00</td>\n",
       "      <td>–≠–≤–∞–∫—É–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å —Å –¥–µ—Ç—å–º–∏ –ø–æ–ø–∞–ª –ø–æ–¥ –æ–±...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68963</th>\n",
       "      <td>–£–∫—Ä–∞–∏–Ω–∞ 24/7 –ù–æ–≤–æ—Å—Ç–∏ | –í–æ–π–Ω–∞ | –ù–æ–≤–∏–Ω–∏</td>\n",
       "      <td>2022-03-26 19:55:59+00:00</td>\n",
       "      <td>‚ùó–ú—ç—Ä –°–ª–∞–≤—É—Ç–∏—á–∞ —Å–æ–æ–±—â–∏–ª, —á—Ç–æ —Ä–∞—à–∏—Å—Ç—ã –æ–∫–∫—É–ø–∏—Ä–æ–≤–∞...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103982</th>\n",
       "      <td>–ü—É–ª N3</td>\n",
       "      <td>2022-05-04 12:07:13+00:00</td>\n",
       "      <td>–£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∏ –∏ –ø–æ–ª–∏—Ç—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏ –æ—á–µ–≤–∏–¥...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38649</th>\n",
       "      <td>–£–ù–ò–ê–ù - –Ω–æ–≤–æ—Å—Ç–∏ –£–∫—Ä–∞–∏–Ω—ã | –≤–æ–π–Ω–∞ —Å –†–æ—Å—Å–∏–µ–π | –Ω–æ...</td>\n",
       "      <td>2022-05-16 07:06:53+00:00</td>\n",
       "      <td>üî•–•–∞—Ä—å–∫–æ–≤ –æ–∂–∏–≤–∞–µ—Ç  –í –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫, 16 –º–∞—è, –≤ –ø–æ—Å...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94986</th>\n",
       "      <td>–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2022-03-23 12:23:52+00:00</td>\n",
       "      <td>‚ö°Ô∏è–ü—É—Ç–∏–Ω: –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –≤ –∫—Ä–∞—Ç—á–∞–π—à–∏–µ —Å—Ä–æ–∫–∏ –ø–µ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  channel  \\\n",
       "73112                          –£–∫—Ä–∞–∏–Ω—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞. –ì–ª–∞–≤–Ω–æ–µ   \n",
       "68963               –£–∫—Ä–∞–∏–Ω–∞ 24/7 –ù–æ–≤–æ—Å—Ç–∏ | –í–æ–π–Ω–∞ | –ù–æ–≤–∏–Ω–∏   \n",
       "103982                                             –ü—É–ª N3   \n",
       "38649   –£–ù–ò–ê–ù - –Ω–æ–≤–æ—Å—Ç–∏ –£–∫—Ä–∞–∏–Ω—ã | –≤–æ–π–Ω–∞ —Å –†–æ—Å—Å–∏–µ–π | –Ω–æ...   \n",
       "94986                                         –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏   \n",
       "\n",
       "                             date  \\\n",
       "73112   2022-05-15 14:45:48+00:00   \n",
       "68963   2022-03-26 19:55:59+00:00   \n",
       "103982  2022-05-04 12:07:13+00:00   \n",
       "38649   2022-05-16 07:06:53+00:00   \n",
       "94986   2022-03-23 12:23:52+00:00   \n",
       "\n",
       "                                                      msg  ukrainian  \n",
       "73112   –≠–≤–∞–∫—É–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å —Å –¥–µ—Ç—å–º–∏ –ø–æ–ø–∞–ª –ø–æ–¥ –æ–±...          1  \n",
       "68963   ‚ùó–ú—ç—Ä –°–ª–∞–≤—É—Ç–∏—á–∞ —Å–æ–æ–±—â–∏–ª, —á—Ç–æ —Ä–∞—à–∏—Å—Ç—ã –æ–∫–∫—É–ø–∏—Ä–æ–≤–∞...          1  \n",
       "103982  –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∏ –∏ –ø–æ–ª–∏—Ç—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏ –æ—á–µ–≤–∏–¥...          0  \n",
       "38649   üî•–•–∞—Ä—å–∫–æ–≤ –æ–∂–∏–≤–∞–µ—Ç  –í –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫, 16 –º–∞—è, –≤ –ø–æ—Å...          1  \n",
       "94986   ‚ö°Ô∏è–ü—É—Ç–∏–Ω: –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –≤ –∫—Ä–∞—Ç—á–∞–π—à–∏–µ —Å—Ä–æ–∫–∏ –ø–µ...          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "preprocessor.read_data()\n",
    "preprocessor.get_data().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b3d636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess took 1.04 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138059, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.preprocess()\n",
    "preprocessor.get_data().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2130e",
   "metadata": {},
   "source": [
    "be carefull; this line runs approx. 2:30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e74b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize took 146.94 seconds.\n"
     ]
    }
   ],
   "source": [
    "preprocessor.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963bb3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize took 7.82 seconds.\n",
      "tfidf_transform took 6.4 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1434042, 110447), (1434042, 27612))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.train_test_split()\n",
    "preprocessor.vectorize(ngram_range=(1,2), binary=True, sublinear_tf=True)\n",
    "X_train, X_test = preprocessor.tfidf_transform()\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01b65f",
   "metadata": {},
   "source": [
    "or preprocessor.count_transform() if CountVectorizer model is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a9f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_cheat_words took 3.23 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['03', '04', '05', '1378', '2022', '3801', '3806', '4149', '4276',\n",
       "       '4279', '9521', '9842', 'akimapachev', 'amp', 'anna', 'com',\n",
       "       'com ua', 'daily', 'daily news', 'diza', 'donbass', 'epoddubny',\n",
       "       'https', 'https www', 'index', 'me', 'me rvvoenkor', 'news',\n",
       "       'opersvodki', 'pravda', 'pravda com', 'rus', 'rus news',\n",
       "       'rvvoenkor', 'sashakots', 'ua', 'ua rus', 'wargonzo',\n",
       "       'wargonzo –Ω–∞—à', 'www', 'www pravda', '–º–∏–¥', '—Ç—Ä—É—Ö–∞',\n",
       "       '—Ç—Ä—É—Ö–∞ —É–∫—Ä–∞–∏–Ω–∞', '—É–∫—Ä–∞–∏–Ω–∞ —Å–µ–π—á–∞—Å'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.remove_cheat_words()\n",
    "mask = preprocessor.get_delete_mask()\n",
    "preprocessor.get_cheat_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f69d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1433997, 110447), (1433997, 27612), (110447,), (27612,), (110447,), (27612,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, ukr_train, ukr_test, channel_train, channel_test = \\\n",
    "preprocessor.get_train_test_split()\n",
    "X_train.shape, X_test.shape, ukr_train.shape, ukr_test.shape, channel_train.shape, channel_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c3445",
   "metadata": {},
   "source": [
    "# An example of using Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f698ef6",
   "metadata": {},
   "source": [
    "Now, let's say we want to predict whether the sentence belongs to ukrainian social media or russian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fb6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '–í—Å–µ –ø–ª–µ–Ω–Ω—ã–µ —Å \"–ê–∑–æ–≤—Å—Ç–∞–ª–∏\" —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –≤ –î–ù–†, \\\n",
    "–∏—Ö –±—É–¥–µ—Ç —Å—É–¥–∏—Ç—å —Ç—Ä–∏–±—É–Ω–∞–ª –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ —Ä–µ—Å–ø—É–±–ª–∏–∫–∏ ‚Äî –≥–ª–∞–≤–∞ –î–ù–† –î–µ–Ω–∏—Å –ü—É—à–∏–ª–∏–Ω.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5258b55",
   "metadata": {},
   "source": [
    "Firstly, need to lemmatiza and transform our sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3011c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize took 0.62 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1433997, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = preprocessor.lemmatize(sentence)\n",
    "vectorizer = preprocessor.get_vectorizer()\n",
    "transformed = vectorizer.transform(lemmatized)[:, ~mask].asfptype().T\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496dcf7",
   "metadata": {},
   "source": [
    "be carefull; this line runs approx. 1 to 60 minutes, depending on k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bc334c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509c3196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_LSA took 138.37 seconds.\n"
     ]
    }
   ],
   "source": [
    "predictor.train_LSA(X_train, ukr_train, k=300)\n",
    "svd = predictor.get_SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "962a8eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_LSA took 126.8 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predictor.predict_LSA(transformed)[0][0]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b3a10",
   "metadata": {},
   "source": [
    "Result is False := **russian**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67ee5f",
   "metadata": {},
   "source": [
    "# Finding the best parameters for 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ed6fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_data took 0.67 seconds.\n",
      "lemmatize took 154.41 seconds.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "preprocessor.read_data()\n",
    "preprocessor.lemmatize()\n",
    "DATA = preprocessor.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdf694",
   "metadata": {},
   "source": [
    "## Latent Sematic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4639f6",
   "metadata": {},
   "source": [
    "We have 2 options for vectorizing method, namely:\n",
    "\n",
    "* CountVectorizer() a.k.a Bag-of-words\n",
    "* TfidfVectorizer() - term-frequency inverse document frequency.\n",
    "\n",
    "In the first cell I'll find the best parameters for BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e6d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize took 2.46 seconds.\n",
      "count_transform took 2.9 seconds.\n",
      "remove_cheat_words took 0.13 seconds.\n",
      "train_LSA took 80.66 seconds.\n",
      "predict_LSA took 0.72 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 1), binary=True, accuracy: 80.05%\n",
      "-----------------------------------------\n",
      "vectorize took 2.46 seconds.\n",
      "count_transform took 2.96 seconds.\n",
      "remove_cheat_words took 0.13 seconds.\n",
      "train_LSA took 79.89 seconds.\n",
      "predict_LSA took 0.66 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 1), binary=False, accuracy: 79.2%\n",
      "-----------------------------------------\n",
      "vectorize took 8.02 seconds.\n",
      "count_transform took 6.38 seconds.\n",
      "remove_cheat_words took 3.61 seconds.\n",
      "train_LSA took 272.23 seconds.\n",
      "predict_LSA took 36.16 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), binary=True, accuracy: 81.72%\n",
      "-----------------------------------------\n",
      "vectorize took 8.1 seconds.\n",
      "count_transform took 6.33 seconds.\n",
      "remove_cheat_words took 3.51 seconds.\n",
      "train_LSA took 296.86 seconds.\n",
      "predict_LSA took 29.24 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), binary=False, accuracy: 80.58%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for ngram_range in [(1, 1), (1, 2)]:\n",
    "    for binary in [True, False]:\n",
    "        preprocessor = Preprocessor(DATA)\n",
    "        preprocessor.train_test_split()\n",
    "        preprocessor.vectorize(ngram_range=ngram_range, binary=binary, sublinear_tf=True)    \n",
    "        preprocessor.count_transform()\n",
    "        preprocessor.remove_cheat_words()\n",
    "        X_train, X_test, ukr_train, ukr_test, channel_train, channel_test = \\\n",
    "        preprocessor.get_train_test_split()\n",
    "\n",
    "        predictor = Predictor()\n",
    "        predictor.train_LSA(X_train, ukr_train, k=500)\n",
    "        ukr_pred = predictor.predict_LSA(X_test)\n",
    "        accuracy = predictor.evaluate(ukr_test)\n",
    "        print('-----------------------------------------')\n",
    "        print(f'ngram={ngram_range}, binary={binary}, accuracy: {accuracy}%')\n",
    "        print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d3503",
   "metadata": {},
   "source": [
    "In the following cell, I'll run the same code, but with Tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74231b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess took 1.31 seconds.\n",
      "vectorize took 7.86 seconds.\n",
      "tfidf_transform took 6.32 seconds.\n",
      "remove_cheat_words took 3.48 seconds.\n",
      "train_LSA took 284.68 seconds.\n",
      "predict_LSA took 22.25 seconds.\n",
      "-----------------------------------------\n",
      "cut_less_than=0, accuracy: 83.58%\n",
      "-----------------------------------------\n",
      "preprocess took 1.35 seconds.\n",
      "vectorize took 7.55 seconds.\n",
      "tfidf_transform took 6.38 seconds.\n",
      "remove_cheat_words took 3.76 seconds.\n",
      "train_LSA took 266.77 seconds.\n",
      "predict_LSA took 18.34 seconds.\n",
      "-----------------------------------------\n",
      "cut_less_than=18, accuracy: 83.68%\n",
      "-----------------------------------------\n",
      "preprocess took 1.36 seconds.\n",
      "vectorize took 7.44 seconds.\n",
      "tfidf_transform took 6.3 seconds.\n",
      "remove_cheat_words took 3.74 seconds.\n",
      "train_LSA took 255.81 seconds.\n",
      "predict_LSA took 21.96 seconds.\n",
      "-----------------------------------------\n",
      "cut_less_than=100, accuracy: 85.91%\n",
      "-----------------------------------------\n",
      "preprocess took 1.38 seconds.\n",
      "vectorize took 4.73 seconds.\n",
      "tfidf_transform took 3.75 seconds.\n",
      "remove_cheat_words took 2.39 seconds.\n",
      "train_LSA took 65.59 seconds.\n",
      "predict_LSA took 6.92 seconds.\n",
      "-----------------------------------------\n",
      "cut_less_than=500, accuracy: 91.14%\n",
      "-----------------------------------------\n",
      "preprocess took 1.31 seconds.\n",
      "vectorize took 2.44 seconds.\n",
      "tfidf_transform took 1.75 seconds.\n",
      "remove_cheat_words took 1.56 seconds.\n",
      "train_LSA took 27.41 seconds.\n",
      "predict_LSA took 2.7 seconds.\n",
      "-----------------------------------------\n",
      "cut_less_than=1000, accuracy: 92.53%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for cut_less_than in [0, 18, 100, 500, 1000]:\n",
    "    preprocessor = Preprocessor(DATA)\n",
    "    preprocessor.preprocess(cut_less_than=cut_less_than)\n",
    "    preprocessor.train_test_split()\n",
    "    preprocessor.vectorize(ngram_range=(1,2), binary=True, sublinear_tf=True)    \n",
    "    preprocessor.tfidf_transform()\n",
    "    preprocessor.remove_cheat_words()\n",
    "    X_train, X_test, ukr_train, ukr_test, channel_train, channel_test = \\\n",
    "    preprocessor.get_train_test_split()\n",
    "\n",
    "    predictor = Predictor()\n",
    "    predictor.train_LSA(X_train, ukr_train, k=500)\n",
    "    ukr_pred = predictor.predict_LSA(X_test)\n",
    "    accuracy = predictor.evaluate(ukr_test)\n",
    "    print('-----------------------------------------')\n",
    "    print(f'cut_less_than={cut_less_than}, accuracy: {accuracy}%')\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc2438",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5eaf6",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier held the worst results (a few percents better than non-negative matrix factorization or k-nearest neighbors), but here are the parameters for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aace7a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize took 2.46 seconds.\n",
      "count_transform took 2.92 seconds.\n",
      "remove_cheat_words took 0.14 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 1), binary=True, accuracy: 56.88%\n",
      "-----------------------------------------\n",
      "vectorize took 8.02 seconds.\n",
      "count_transform took 6.65 seconds.\n",
      "remove_cheat_words took 3.44 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), binary=True, accuracy: 56.88%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for ngram_range in [(1, 1), (1, 2)]:\n",
    "    preprocessor = Preprocessor(DATA)\n",
    "    preprocessor.train_test_split()\n",
    "    preprocessor.vectorize(ngram_range=ngram_range, binary=True, sublinear_tf=True)    \n",
    "    preprocessor.count_transform()\n",
    "    preprocessor.remove_cheat_words()\n",
    "    X_train, X_test, ukr_train, ukr_test, channel_train, channel_test = \\\n",
    "    preprocessor.get_train_test_split()\n",
    "\n",
    "    predictor = Predictor()\n",
    "    predictor.train_NBC(X_train, ukr_train)\n",
    "    ukr_pred = predictor.predict_NBC(X_test)\n",
    "    accuracy = predictor.evaluate(ukr_test)\n",
    "    print('-----------------------------------------')\n",
    "    print(f'ngram={ngram_range}, binary={True}, accuracy: {accuracy}%')\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89f41f",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f536bb6",
   "metadata": {},
   "source": [
    "The same as with LSA model, here we'll test 2 situations:\n",
    "\n",
    "* for CountVectorizer\n",
    "* for TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f9d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess took 1.28 seconds.\n",
      "vectorize took 7.68 seconds.\n",
      "count_transform took 6.17 seconds.\n",
      "remove_cheat_words took 3.51 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), cut_less_than=0, accuracy: 89.92%\n",
      "-----------------------------------------\n",
      "preprocess took 1.29 seconds.\n",
      "vectorize took 7.62 seconds.\n",
      "count_transform took 6.06 seconds.\n",
      "remove_cheat_words took 3.45 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), cut_less_than=18, accuracy: 90.55%\n",
      "-----------------------------------------\n",
      "preprocess took 1.28 seconds.\n",
      "vectorize took 7.54 seconds.\n",
      "count_transform took 6.08 seconds.\n",
      "remove_cheat_words took 3.32 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), cut_less_than=100, accuracy: 91.26%\n",
      "-----------------------------------------\n",
      "preprocess took 1.28 seconds.\n",
      "vectorize took 4.8 seconds.\n",
      "count_transform took 3.59 seconds.\n",
      "remove_cheat_words took 2.76 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), cut_less_than=500, accuracy: 93.15%\n",
      "-----------------------------------------\n",
      "preprocess took 1.28 seconds.\n",
      "vectorize took 2.32 seconds.\n",
      "count_transform took 1.59 seconds.\n",
      "remove_cheat_words took 1.24 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), cut_less_than=1000, accuracy: 93.97%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for cut_less_than in [0, 18, 100, 500, 1000]:\n",
    "    preprocessor = Preprocessor(DATA)\n",
    "    preprocessor.preprocess(cut_less_than=cut_less_than)\n",
    "    preprocessor.train_test_split()\n",
    "    preprocessor.vectorize(ngram_range=ngram_range, binary=True, sublinear_tf=True)    \n",
    "    preprocessor.count_transform()\n",
    "    preprocessor.remove_cheat_words()\n",
    "    X_train, X_test, ukr_train, ukr_test, channel_train, channel_test = \\\n",
    "    preprocessor.get_train_test_split()\n",
    "\n",
    "    predictor = Predictor()\n",
    "    predictor.train_LR(X_train, ukr_train)\n",
    "    ukr_pred = predictor.predict_LR(X_test)\n",
    "    accuracy = predictor.evaluate(ukr_test)\n",
    "    print('-----------------------------------------')\n",
    "    print(f'ngram={ngram_range}, cut_less_than={cut_less_than}, accuracy: {accuracy}%')\n",
    "    print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297ef75",
   "metadata": {},
   "source": [
    "This section is for TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1865cf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize took 2.44 seconds.\n",
      "tfidf_transform took 2.97 seconds.\n",
      "remove_cheat_words took 0.14 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 1), binary=True, accuracy: 86.59%\n",
      "-----------------------------------------\n",
      "vectorize took 2.44 seconds.\n",
      "tfidf_transform took 2.97 seconds.\n",
      "remove_cheat_words took 0.13 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 1), binary=False, accuracy: 86.57%\n",
      "-----------------------------------------\n",
      "vectorize took 8.01 seconds.\n",
      "tfidf_transform took 6.74 seconds.\n",
      "remove_cheat_words took 3.49 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), binary=True, accuracy: 88.75%\n",
      "-----------------------------------------\n",
      "vectorize took 8.02 seconds.\n",
      "tfidf_transform took 6.77 seconds.\n",
      "remove_cheat_words took 3.56 seconds.\n",
      "-----------------------------------------\n",
      "ngram=(1, 2), binary=False, accuracy: 88.7%\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_pred = 0\n",
    "best_options = [(1, 1), True, None]\n",
    "for ngram_range in [(1, 1), (1, 2)]:\n",
    "    for binary in [True, False]:\n",
    "        preprocessor = Preprocessor(DATA)\n",
    "        preprocessor.train_test_split()\n",
    "        preprocessor.vectorize(ngram_range=ngram_range, binary=binary, sublinear_tf=True)    \n",
    "        preprocessor.tfidf_transform()\n",
    "        preprocessor.remove_cheat_words()\n",
    "        X_train, X_test, ukr_train, ukr_test, channel_train, channel_test = \\\n",
    "        preprocessor.get_train_test_split()\n",
    "\n",
    "        predictor = Predictor()\n",
    "        predictor.train_LR(X_train, ukr_train)\n",
    "        ukr_pred = predictor.predict_LR(X_test)\n",
    "        accuracy = predictor.evaluate(ukr_test)\n",
    "        print('-----------------------------------------')\n",
    "        print(f'ngram={ngram_range}, binary={binary}, accuracy: {accuracy}%')\n",
    "        print('-----------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
