{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-GkiXWjq3kR"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Au8jljOEq3kT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from pymystem3 import Mystem\n",
    "import plotly.express as px\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from sys import getsizeof\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "SEED = 1003\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoHvbO3Fq3kU"
   },
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H5_8CtWTq3kV"
   },
   "outputs": [],
   "source": [
    "UKR_CHANNELS = [\n",
    "    '–¢—Ä—É—Ö–∞‚ö°Ô∏è–£–∫—Ä–∞–∏–Ω–∞', '–õ–∞—á–µ–Ω –ø–∏—à–µ—Ç', '–£–∫—Ä–∞–∏–Ω—Å–∫–∞—è –ø—Ä–∞–≤–¥–∞. –ì–ª–∞–≤–Ω–æ–µ',\n",
    "    '–í—ã —Ö–æ—Ç–∏—Ç–µ –∫–∞–∫ –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ?', '–ë–æ—Ä–∏—Å –§—ñ–ª–∞—Ç–æ–≤', 'RAGNAROCK PRIVET',\n",
    "    '–£–ù–ò–ê–ù - –Ω–æ–≤–æ—Å—Ç–∏ –£–∫—Ä–∞–∏–Ω—ã | –≤–æ–π–Ω–∞ —Å –†–æ—Å—Å–∏–µ–π | –Ω–æ–≤–∏–Ω–∏ –£–∫—Ä–∞—ó–Ω–∏ | –≤—ñ–π–Ω–∞ –∑ –†–æ—Å—ñ—î—é',\n",
    "    '–£–∫—Ä–∞–∏–Ω–∞ 24/7 –ù–æ–≤–æ—Å—Ç–∏ | –í–æ–π–Ω–∞ | –ù–æ–≤–∏–Ω–∏', '–ë—ã—Ç—å –ò–ª–∏',\n",
    "    '–£–∫—Ä–∞–∏–Ω–∞ –°–µ–π—á–∞—Å: –Ω–æ–≤–æ—Å—Ç–∏, –≤–æ–π–Ω–∞, –†–æ—Å—Å–∏—è'\n",
    "]\n",
    "\n",
    "UKR_LETTERS = ['—ó', '—î', '“ë', '—ñ']\n",
    "\n",
    "CHEAT_WORDS = [\n",
    "    '03', '04', '05', '1378', '2022', '3801', '3806', '4149', '4276',\n",
    "    '4279', '9521', '9842', 'akimapachev', 'amp', 'anna', 'com',\n",
    "    'daily', 'diza', 'donbass', 'epoddubny', 'https', 'index', 'me',\n",
    "    'news', 'opersvodki', 'pravda', 'rus', 'rvvoenkor', 'sashakots',\n",
    "    'ua', 'wargonzo', 'www', 'www pravda', '–º–∏–¥', '—Ç—Ä—É—Ö–∞', '—Ç—Ä—É—Ö–∞ —É–∫—Ä–∞–∏–Ω–∞',\n",
    "    '—É–∫—Ä–∞–∏–Ω–∞ —Å–µ–π—á–∞—Å', 'pravda com', 'daily news', 'com ua', 'https www',\n",
    "    'me rvvoenkor', 'rus news', 'ua rus', 'wargonzo –Ω–∞—à'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e_eWDTZaq3kV"
   },
   "outputs": [],
   "source": [
    "def time_decorator(function):\n",
    "    from time import time\n",
    "    def inner(*args, **kwargs):\n",
    "        start = time()\n",
    "        result = function(*args, **kwargs)\n",
    "        elapsed_time = round(time() - start, 2)\n",
    "        output = f'{function.__name__} took {elapsed_time} seconds.'\n",
    "        print(output)\n",
    "        return result\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5aLTy_rq3kW"
   },
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ki6EI2Pfq3kW"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def read_data(filename='random_msgs.csv', sep='¬∂‚àÜ',\n",
    "                  header=None):\n",
    "    \"\"\"\n",
    "    Reads the csv file into 4 columns:\n",
    "    channel\n",
    "    date of publication\n",
    "    message\n",
    "    ukrainian - 1 if ukrainian channel, 0 - otherwise.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename, sep=sep, header=header)\n",
    "    data.columns = ['channel', 'date', 'msg']\n",
    "    data['ukrainian'] = data['channel'].\\\n",
    "        apply(lambda x: 1 if x in UKR_CHANNELS else 0)\n",
    "    data['ukrainian'] = data['ukrainian'].astype('int8')\n",
    "    data = data[data['channel'] != '–≤–µ—á–µ—Ä—è—î–º–æ']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "hM2zbIS6q3kW",
    "outputId": "a5ec5456-e070-4818-e538-a80b16d4c70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_data took 0.64 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>date</th>\n",
       "      <th>msg</th>\n",
       "      <th>ukrainian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18031</th>\n",
       "      <td>Zvezdanews</td>\n",
       "      <td>2022-03-18 09:35:46+00:00</td>\n",
       "      <td>‚ö°Ô∏è–ë–µ—Å–µ–¥—É –ü—É—Ç–∏–Ω–∞ —Å –®–æ–ª—å—Ü–µ–º –≤—Ä—è–¥ –ª–∏ –º–æ–∂–Ω–æ –Ω–∞–∑–≤–∞—Ç...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48869</th>\n",
       "      <td>üá∑üá∫ –ú–ò–ì üåç</td>\n",
       "      <td>2022-02-24 16:48:23+00:00</td>\n",
       "      <td>–ù–∞–¥–æ –Ω–∞–∑—ã–≤–∞—Ç—å –≤–µ—â–∏ —Å–≤–æ–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ - —ç—Ç–æ –Ω–µ \"–Ω–µ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123905</th>\n",
       "      <td>–í—ã —Ö–æ—Ç–∏—Ç–µ –∫–∞–∫ –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ?</td>\n",
       "      <td>2022-03-30 14:43:40+00:00</td>\n",
       "      <td>‚ö°Ô∏è–í –ë–µ–ª–æ–º –¥–æ–º–µ —Å–æ–æ–±—â–∏–ª–∏, —á—Ç–æ –ë–∞–π–¥–µ–Ω –ø—Ä–æ–≤–µ–¥–µ—Ç —Ç...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128482</th>\n",
       "      <td>Kotsnews</td>\n",
       "      <td>2022-04-08 08:31:06+00:00</td>\n",
       "      <td>–•–∏—Ç—ã –¥–µ—Ä–µ–≤–µ–Ω—Å–∫–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫. ‚Äú–£–∫—Ä–∞–∏–Ω–∞. –•—Ä–æ–Ω–æ–ª–æ–≥...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136038</th>\n",
       "      <td>ZERGULIOüá∑üá∫</td>\n",
       "      <td>2022-05-05 05:04:36+00:00</td>\n",
       "      <td>46 –ª–µ—Ç –∏ —Ç–∞–∫–æ–π –∏–¥–∏–æ—Ç, —Ç—Ä–∞–≥–µ–¥–∏—è –ø—Ä–æ—Å—Ç–æ https://...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          channel                       date  \\\n",
       "18031                  Zvezdanews  2022-03-18 09:35:46+00:00   \n",
       "48869                    üá∑üá∫ –ú–ò–ì üåç  2022-02-24 16:48:23+00:00   \n",
       "123905  –í—ã —Ö–æ—Ç–∏—Ç–µ –∫–∞–∫ –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ?  2022-03-30 14:43:40+00:00   \n",
       "128482                   Kotsnews  2022-04-08 08:31:06+00:00   \n",
       "136038                 ZERGULIOüá∑üá∫  2022-05-05 05:04:36+00:00   \n",
       "\n",
       "                                                      msg  ukrainian  \n",
       "18031   ‚ö°Ô∏è–ë–µ—Å–µ–¥—É –ü—É—Ç–∏–Ω–∞ —Å –®–æ–ª—å—Ü–µ–º –≤—Ä—è–¥ –ª–∏ –º–æ–∂–Ω–æ –Ω–∞–∑–≤–∞—Ç...          0  \n",
       "48869   –ù–∞–¥–æ –Ω–∞–∑—ã–≤–∞—Ç—å –≤–µ—â–∏ —Å–≤–æ–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ - —ç—Ç–æ –Ω–µ \"–Ω–µ...          0  \n",
       "123905  ‚ö°Ô∏è–í –ë–µ–ª–æ–º –¥–æ–º–µ —Å–æ–æ–±—â–∏–ª–∏, —á—Ç–æ –ë–∞–π–¥–µ–Ω –ø—Ä–æ–≤–µ–¥–µ—Ç —Ç...          1  \n",
       "128482  –•–∏—Ç—ã –¥–µ—Ä–µ–≤–µ–Ω—Å–∫–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫. ‚Äú–£–∫—Ä–∞–∏–Ω–∞. –•—Ä–æ–Ω–æ–ª–æ–≥...          0  \n",
       "136038  46 –ª–µ—Ç –∏ —Ç–∞–∫–æ–π –∏–¥–∏–æ—Ç, —Ç—Ä–∞–≥–µ–¥–∏—è –ø—Ä–æ—Å—Ç–æ https://...          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_data('random_msgs.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-1SjCMrq3kX",
    "outputId": "12dfa0d2-010b-4f26-8bd4-cd5df4a2757f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukrainian media data percentage: 0.4295310285326924\n",
      "Russian media data percentage: 0.5704689714673076\n"
     ]
    }
   ],
   "source": [
    "percent_ukr = data['ukrainian'].mean()\n",
    "percent_rus = 1 - percent_ukr\n",
    "print(\"Ukrainian media data percentage:\",percent_ukr)\n",
    "print(\"Russian media data percentage:\",percent_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAhGJGUpq3kX"
   },
   "source": [
    "# Preprocessing (removal of Ukrainian language posts, removal of short posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6rA_dcisq3kY"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def preprocess(data, remove_ukr_msgs=True, cut_less_than=18):\n",
    "    \"\"\"\n",
    "    This method:\n",
    "    removes short messages (with less than 18 characters);\n",
    "    removes messages with ukrainian letters.\n",
    "    \"\"\"\n",
    "    if remove_ukr_msgs:\n",
    "        for letter in UKR_LETTERS:\n",
    "            data = data[data['msg'].str.lower().\\\n",
    "                                    str.contains(letter) == False]\n",
    "    data = data[data['msg'].str.len() > cut_less_than]\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSSHXPXAq3kY",
    "outputId": "0184ff9d-8491-4b1d-dee4-2283fc14ab04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess took 1.04 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138059, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = preprocess(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nc6FxtZq3kY",
    "outputId": "af1c9c52-2814-4e93-f923-878792a87ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukrainian media data percentage: 0.4014515533214061\n",
      "Russian media data percentage: 0.598548446678594\n"
     ]
    }
   ],
   "source": [
    "percent_ukr = data['ukrainian'].mean()\n",
    "percent_rus = 1 - percent_ukr\n",
    "print(\"Ukrainian media data percentage:\",percent_ukr)\n",
    "print(\"Russian media data percentage:\",percent_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ftjnxq9q3kY"
   },
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tOpfv6b4q3kY"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def lemmatize(data, *sentences):\n",
    "    \"\"\"\n",
    "    This method has 2 usages:\n",
    "    internal; i.e. to lemmatize all messages in the dataset. Runs about 2.5\n",
    "    minutes.\n",
    "    outside; to lemmatize a given sequence of sentences.\n",
    "    \"\"\"\n",
    "    mystem = Mystem()\n",
    "    if not sentences:\n",
    "        def preprocess_text(text):\n",
    "            tokens = mystem.lemmatize(text.lower())\n",
    "            text = \" \".join(tokens)\n",
    "            return text\n",
    "\n",
    "        data['msg'] = data['msg'].apply(preprocess_text)\n",
    "        return data\n",
    "    else:\n",
    "        result = []\n",
    "        for sentence in sentences:\n",
    "            tokens = mystem.lemmatize(sentence.lower())\n",
    "            result.append(' '.join(tokens))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJgKufpCq3kZ",
    "outputId": "a6005f97-7124-4888-eb46-07ec7453085c"
   },
   "outputs": [],
   "source": [
    "data = lemmatize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tXie8Egq3kZ"
   },
   "source": [
    "# Train, test split\n",
    "**80, 10, 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CbUzMAqq3kZ"
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(data, random_state=1, train_size=.8):\n",
    "    \"\"\"\n",
    "    This method clones scikit-learn train_test_split.\n",
    "    \"\"\"\n",
    "    X_train, X_rest, ukr_train, ukr_rest, channel_train, channel_rest = \\\n",
    "    train_test_split(\n",
    "        data['msg'], data['ukrainian'], data['channel'],\n",
    "        random_state=random_state, train_size=train_size\n",
    "    )\n",
    "    \n",
    "    X_val, X_test, ukr_val, ukr_test, channel_val, channel_test = \\\n",
    "    train_test_split(\n",
    "        X_rest, ukr_rest, channel_rest,\n",
    "        random_state=random_state, train_size=.5\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, ukr_train.values, ukr_val.values, ukr_test.values, channel_train, channel_val, channel_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hd3bBG_gq3kZ"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, ukr_train, ukr_val, ukr_test, channel_train, channel_val, channel_test = train_val_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nmf8p7vEq3kZ",
    "outputId": "a8142fda-382d-4fd4-ea65-dae4304de4ab"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_FPDC6dq3ka"
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpUxWDd4q3ka"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def vectorize(X_train, ngram_range=(1,1), sublinear_tf=True, binary=False):\n",
    "    \"\"\"\n",
    "    This method creates a pipeline of CountVectorizer() and TfidfTransformer().\n",
    "    If CountVectorizer is needed - use count_transform method.\n",
    "    If TfidfVectorizer is needed - just call a tfidf_transform method.\n",
    "    \"\"\"\n",
    "    tfidf = Pipeline([\n",
    "                ('vect', CountVectorizer(binary=binary, ngram_range=ngram_range)),\n",
    "                ('tfidf', TfidfTransformer(sublinear_tf=sublinear_tf))\n",
    "            ]).fit(X_train)\n",
    "    \n",
    "    vect = tfidf['vect']\n",
    "    return tfidf, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UvLH5C7q3ka",
    "outputId": "760a8951-3088-4fbc-a72b-d06ef72e2188"
   },
   "outputs": [],
   "source": [
    "tfidf, vect = vectorize(X_train, ngram_range=(1,2), sublinear_tf=True, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsQij55jq3ka"
   },
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYIcppyiq3ka"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def tfidf_transform(tfidf, *X):\n",
    "    \"\"\"\n",
    "    Applies TfidfTransform to data.\n",
    "    \"\"\"\n",
    "    return [tfidf.transform(x).T for x in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NVIuIWiq3ka",
    "outputId": "b617a58b-25c0-44fe-dba2-a1932e2a8db0"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = tfidf_transform(tfidf, X_train, X_val, X_test)\n",
    "X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5VcpO18q3kb"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def count_transform(vect, *X):\n",
    "    \"\"\"\n",
    "    Applies CountTransform to data.\n",
    "    \"\"\"\n",
    "    return [vect.transform(x).T for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count, X_val_count, X_test_count = count_transform(vect, X_train, X_val, X_test)\n",
    "X_train_count.shape, X_val_count.shape, X_test_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GATKxmoq3kb"
   },
   "source": [
    "# Remove cheat words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_Wrb8Vbq3kb"
   },
   "source": [
    "or preprocessor.count_transform() if CountVectorizer model is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYh8puF0q3kb"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def remove_cheat_words(X_train, X_val, X_test, channel_train, vectorizer, method='manual', freq_pivot=.5,\n",
    "                           cheat_words=CHEAT_WORDS):\n",
    "    \"\"\"\n",
    "    Removes cheat_words, like channel tags, social media links or\n",
    "    authors names.\n",
    "    \"\"\"\n",
    "    if method == 'manual':\n",
    "        delete_mask = np.zeros(X_train.shape[0], dtype=bool)\n",
    "        delete_mask[np.isin(np.array(\n",
    "                vectorizer.get_feature_names_out()), cheat_words)\n",
    "        ] = True\n",
    "        X_train = X_train.T[:, ~delete_mask].T\n",
    "        X_val = X_val.T[:, ~delete_mask].T\n",
    "        X_test = X_test.T[:, ~delete_mask].T\n",
    "        cheat_words = np.array(vectorizer.get_feature_names_out() ).T[delete_mask]\n",
    "\n",
    "    else:\n",
    "        delete_mask = np.zeros(X_train.shape[0], dtype=bool)\n",
    "        for channel in channel_train.unique():\n",
    "            arr = X_train.T[channel_train == channel]\n",
    "            delete_mask |= np.array((np.sum(arr > 0, axis=0) / arr.shape[0]) > .5)[0]\n",
    "\n",
    "        X_train = X_train.T[:, ~delete_mask].T\n",
    "        X_val = X_val.T[:, ~delete_mask].T\n",
    "        X_test = X_test.T[:, ~delete_mask].T\n",
    "        delete_mask = delete_mask\n",
    "        cheat_words = np.array(\n",
    "            vectorizer.get_feature_names_out()\n",
    "        ).T[delete_mask]\n",
    "        \n",
    "    return X_train, X_val, X_test, delete_mask    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjlkMwzOq3kb",
    "outputId": "fdee35b9-87b5-4fe5-b76e-20b809df4dc0"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, mask = remove_cheat_words(X_train_tfidf, X_val_tfidf, X_test_tfidf, channel_train, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2qVyzYLq3kb",
    "outputId": "bad08859-4646-4be8-f22b-0b57eaa29bb5"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3gOAPSuq3kc"
   },
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulrPVEfPq3kc"
   },
   "source": [
    "Now, let's say we want to predict whether the sentence belongs to ukrainian social media or russian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mOvil-GQq3kc"
   },
   "outputs": [],
   "source": [
    "sentence = '–í—Å–µ –ø–ª–µ–Ω–Ω—ã–µ —Å \"–ê–∑–æ–≤—Å—Ç–∞–ª–∏\" —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –≤ –î–ù–†, \\\n",
    "–∏—Ö –±—É–¥–µ—Ç —Å—É–¥–∏—Ç—å —Ç—Ä–∏–±—É–Ω–∞–ª –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ —Ä–µ—Å–ø—É–±–ª–∏–∫–∏ ‚Äî –≥–ª–∞–≤–∞ –î–ù–† –î–µ–Ω–∏—Å –ü—É—à–∏–ª–∏–Ω.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-mvRBPZq3kc"
   },
   "source": [
    "Firstly, need to lemmatiza and transform our sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jD4X1lMMq3kc",
    "outputId": "e7d7ff0d-037a-49f3-903f-90bffe2dbc92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize took 0.58 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1433997, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = lemmatize(None, sentence)\n",
    "vectorizer = tfidf\n",
    "transformed = vectorizer.transform(lemmatized)[:, ~mask].asfptype().T\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gwLFRc3q3kc"
   },
   "source": [
    "be carefull; this line runs approx. 1 to 60 minutes, depending on k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zjRWinf5q3kd"
   },
   "outputs": [],
   "source": [
    "@time_decorator\n",
    "def train_LSA(X_train, ukr_train, k=150):\n",
    "    Terms, S, Documents = svds(X_train, k=k)\n",
    "    ukr_centre = np.array([np.mean(Documents.T[ukr_train == 1], axis=0)])\n",
    "    rus_centre = np.array([np.mean(Documents.T[ukr_train == 0], axis=0)])\n",
    "    return Terms, S, Documents, ukr_centre, rus_centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_LSA took 127.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "K = 300\n",
    "Terms, S, Documents, ukr_centre, rus_centre = train_LSA(X_train, ukr_train, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1433997, 300), (300,), (300, 110447))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Terms.shape, S.shape, Documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \n",
    "    @time_decorator\n",
    "    def __init__(self, X, y):\n",
    "        global Terms, S, Documents\n",
    "        self.X = torch.from_numpy(np.diag(1 / S) @ Terms.T @ X).T\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ took 130.5 seconds.\n",
      "__init__ took 178.94 seconds.\n",
      "__init__ took 128.45 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSet(X_train, ukr_train)\n",
    "val_dataset = DataSet(X_val, ukr_val)\n",
    "test_dataset = DataSet(X_test, ukr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_dataset, batch_size=1_000, shuffle=True)\n",
    "val = DataLoader(val_dataset, batch_size=1_000, shuffle=True)\n",
    "test = DataLoader(test_dataset, batch_size=1_000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout=.25):\n",
    "        super().__init__()\n",
    "        global K\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.fc1 = nn.Linear(K, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc3(x)).view(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "L = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    global train, L, optimizer, model\n",
    "    train_loss = 0.\n",
    "\n",
    "    for i, (x, labels) in enumerate(train):\n",
    "        out = model(x)\n",
    "        loss = L(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "82.7%\n",
      "LOSS train 0.500 valid 0.403\n",
      "EPOCH 6:\n",
      "84.0%\n",
      "LOSS train 0.353 valid 0.341\n",
      "EPOCH 11:\n",
      "84.0%\n",
      "LOSS train 0.341 valid 0.345\n",
      "EPOCH 16:\n",
      "84.6%\n",
      "LOSS train 0.336 valid 0.331\n",
      "EPOCH 21:\n",
      "84.8%\n",
      "LOSS train 0.332 valid 0.331\n",
      "EPOCH 26:\n",
      "85.0%\n",
      "LOSS train 0.328 valid 0.326\n",
      "EPOCH 31:\n",
      "85.1%\n",
      "LOSS train 0.325 valid 0.325\n",
      "EPOCH 36:\n",
      "84.9%\n",
      "LOSS train 0.325 valid 0.328\n",
      "EPOCH 41:\n",
      "85.0%\n",
      "LOSS train 0.323 valid 0.328\n",
      "EPOCH 46:\n",
      "85.2%\n",
      "LOSS train 0.322 valid 0.325\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = train_one_epoch(epoch)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for i, (x, labels) in enumerate(val):\n",
    "        out = model(x)\n",
    "        vloss = L(out, labels)\n",
    "        val_loss += vloss.item()\n",
    "    \n",
    "    x, labels = val_dataset[:]\n",
    "    out = model(x)\n",
    "    \n",
    "    if epoch % (EPOCHS // 10) == 0:\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        print(f'{100 * (out.round() == labels).float().mean().item():.1f}%')\n",
    "        print('LOSS train {:.3f} valid {:.3f}'.format(train_loss, val_loss / (i + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'DL_model_on_TF-IDF_PCA.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset, val_dataset, test_dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
